from typing import Sequence, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph
import operator
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field
from typing import List, Optional
from typing_extensions import TypedDict
from loguru import logger

from aimode.core.tools import sql_query_generator, execute_sql_query, generate_testplan, set_current_session_id
from aimode.core.prompts import AGENT_PROMPT, SUGGESTION_LLM_PROMPT
from aimode.core.llms import llm


# State definition
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    user_prompt: Optional[str]
    session_id: str  

# Structured output model
class GetSuggestions(BaseModel):
    base_content: str = Field(..., description="Raw base content generated by LLM")
    suggestions: Optional[List[str]] = Field(None, description="List of suggested options")

# Bind tools
tools = [sql_query_generator, execute_sql_query, generate_testplan]
llm_with_tools = AGENT_PROMPT | llm.bind_tools(tools)

# Chatbot node
def chatbot(state: AgentState):
    messages = state['messages']
    user_prompt = state.get('user_prompt', None)
    session_id = state.get('session_id', None)

    logger.info(f"[agent.py] chatbot called with session_id={session_id}, messages_count={len(messages)}")

    if not session_id:
        raise ValueError("session_id missing from state")

    if not user_prompt and messages:
        user_prompt = messages[0].content if hasattr(messages[0], 'content') else str(messages[0])

    # Set session_id for tools globally
    set_current_session_id(session_id)
    logger.info(f"[agent.py] session_id set for tools: {session_id}")

    llm_input = {
        "messages": messages,
        "user_prompt": user_prompt
    }

    response = llm_with_tools.invoke(llm_input)
    content_to_structure = response.content if hasattr(response, 'content') else str(response)

    chain = SUGGESTION_LLM_PROMPT | llm.with_structured_output(
        GetSuggestions,
        method="function_calling"
    )
    structured = chain.invoke({"content": content_to_structure})

    # Attach structured output and session info
    if hasattr(response, 'additional_kwargs'):
        response.additional_kwargs["structured"] = structured.dict()
        response.additional_kwargs["user_prompt"] = user_prompt
        response.additional_kwargs["session_id"] = session_id
    else:
        response.structured = structured.dict()
        response.user_prompt = user_prompt
        response.session_id = session_id

    return {'messages': [response]}

# Workflow definition
workflow = StateGraph(AgentState)
workflow.add_node("chatbot", chatbot)
workflow.add_node("tools", ToolNode(tools))
workflow.add_conditional_edges("chatbot", tools_condition)
workflow.add_edge("tools", "chatbot")
workflow.set_entry_point("chatbot")

memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

