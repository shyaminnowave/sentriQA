from typing import Sequence, Annotated
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph
import operator
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel, Field
from typing import List, Optional
from typing_extensions import TypedDict
from loguru import logger

from aimode.core.tools import sql_query_generator, execute_sql_query, generate_testplan, save_new_testplan_version, set_current_session_id, filter_testcases_tool
from aimode.core.prompts import AGENT_PROMPT, SUGGESTION_LLM_PROMPT
from aimode.core.helpers import get_sql_table_names
from aimode.core.llms import llm


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    user_prompt: Optional[str]
    session_id: str  

class GetSuggestions(BaseModel):
    base_content: str = Field(..., description="Raw base content generated by LLM")
    suggestions: Optional[List[str]] = Field(None, description="List of suggested options")

tools = [sql_query_generator, execute_sql_query, generate_testplan, save_new_testplan_version, filter_testcases_tool]
llm_with_tools = AGENT_PROMPT | llm.bind_tools(tools)


def chatbot(state: AgentState):
    messages = state['messages']
    user_prompt = state.get('user_prompt', None)
    session_id = state.get('session_id', None)

    logger.info(f"chatbot called with session_id={session_id}, messages_count={len(messages)}")

    if not session_id:
        raise ValueError("session_id missing from state")

    if not user_prompt and messages:
        user_prompt = messages[0].content if hasattr(messages[0], 'content') else str(messages[0])

    set_current_session_id(session_id=session_id, user_prompt=user_prompt)
    logger.info(f"session_id set for tools: {session_id}")
    response = llm_with_tools.invoke({"messages": messages})
    # logger.info(f"LLM raw response: {response}")
    if hasattr(response, 'additional_kwargs'):
        logger.info(f"LLM additional kwargs: {response.additional_kwargs}")

    content_to_structure = response.content if hasattr(response, 'content') else str(response)

    chain = SUGGESTION_LLM_PROMPT | llm.with_structured_output(
        GetSuggestions,
        method="function_calling"
    )
    structured = chain.invoke({"content": content_to_structure})
    try:
        structured_dict = structured.dict()
        base_content = structured_dict.get("base_content")
        suggestions = structured_dict.get("suggestions", [])

        logger.info(" --- Structured Output ---")
        logger.info(f"Base content: {base_content}")
        logger.info(f"Suggestions extracted: {suggestions}")
        logger.debug(f"Full structured object: {structured_dict}")
    except Exception as e:
        logger.error(f"Failed to log structured suggestions: {e}")

    if hasattr(response, 'additional_kwargs'):
        response.additional_kwargs["structured"] = structured.dict()
        response.additional_kwargs["user_prompt"] = user_prompt
        response.additional_kwargs["session_id"] = session_id
    else:
        response.structured = structured.dict()
        response.user_prompt = user_prompt
        response.session_id = session_id

    return {'messages': [response]}

# Workflow definition
workflow = StateGraph(AgentState)
workflow.add_node("chatbot", chatbot)
workflow.add_node("tools", ToolNode(tools))
workflow.add_conditional_edges("chatbot", tools_condition)
workflow.add_edge("tools", "chatbot")
workflow.set_entry_point("chatbot")

memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)
